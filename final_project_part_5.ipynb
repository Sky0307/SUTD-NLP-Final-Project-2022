{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf771d3",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5007808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b1db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"dataset\" # dir where your train/dev.in is stored\n",
    "FEATURE_OUTPUT_DIRECTORY = \"features\"\n",
    "LEARN_CO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e62f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_file(directory):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    all_tags = []\n",
    "    all_words = []\n",
    "\n",
    "    with open(directory) as f:\n",
    "        x_sent = []\n",
    "        y = []\n",
    "        for line in f:\n",
    "            if line == '\\n': # end of a sentence\n",
    "                x_train.append(x_sent)\n",
    "                y_train.append(y)\n",
    "                x_sent=[]\n",
    "                y=[]\n",
    "            else:\n",
    "                temp = line.strip().split()\n",
    "                x_sent.append(temp[0]) # word\n",
    "                y.append(temp[1]) # tag\n",
    "\n",
    "                if temp[1] not in all_tags:\n",
    "                    all_tags.append(temp[1])\n",
    "                if temp[0] not in all_words:\n",
    "                    all_words.append(temp[0])\n",
    "\n",
    "    return x_train, y_train, all_tags, all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c10b9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, ALL_TAGS,  ALL_WORDS = read_train_file(DATA_FILE + '/train')\n",
    "all_tags_len = len(ALL_TAGS)\n",
    "# x_train\n",
    "# ALL_TAGS # without START, STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b1a81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_feature(feat, output_file):\n",
    "    with open(output_file, \"w\") as out:\n",
    "        for (k, v) in feat.items():\n",
    "            out.write(f\"{k} {v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c67c3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict_part5(x_train, y_train, output_dir):\n",
    "    \n",
    "    features = {}\n",
    "    tags = ALL_TAGS + [\"START\", \"STOP\"] \n",
    "    unk = \"<UNK>\"\n",
    "    all_words = ALL_WORDS +[unk]\n",
    "    #Initiate transition\n",
    "    for i in tags:\n",
    "        for j in tags:\n",
    "            for e in all_words:\n",
    "                string = f\"transition:{str(i)}+{str(j)}+{str(e)}\"\n",
    "                features[string] = -2**21\n",
    "    \n",
    "    #Initiate emission dictionaries 1 and 2\n",
    "    for j in ALL_TAGS:\n",
    "        for i in ALL_WORDS:\n",
    "            string = f\"emission1:{str(j)}+{str(i)}\"\n",
    "            features[string] = -2**21\n",
    "\n",
    "    for j in ALL_TAGS:\n",
    "        for i in ALL_WORDS:\n",
    "            string = f\"emission2:{str(j)}+{str(i)}\"\n",
    "            features[string] = -2**21\n",
    "    \n",
    "    #Initiate transition2\n",
    "    for i in tags:\n",
    "        for j in tags:\n",
    "            string = f\"transition2:{str(i)}+{str(j)}\"\n",
    "            features[string] = -2**21\n",
    "    \n",
    "    #Initiate emission dictionaries 3\n",
    "    for j in ALL_TAGS:\n",
    "        for i in ALL_WORDS:\n",
    "            string = f\"emission3:{str(j)}+{str(i)}\"\n",
    "            features[string] = -2**21\n",
    "\n",
    "\n",
    "    #Initiate dictionary for counting emissions\n",
    "    label_dict = defaultdict(int)  # {LABEL : COUNT} e.g: {'o': 24273, 'B-negative': 278, ...}\n",
    "    word_label_dict_prev = defaultdict(int) # {(LABEL, WORD): COUNT} \n",
    "                               # e.g: {('O', 'All'): 3, ('B-positive', 'food'): 131, ...}\n",
    "    word_label_dict_next = defaultdict(int) \n",
    "\n",
    "    label_dict2 = defaultdict(int)  # {LABEL : COUNT} e.g: {'o': 24273, 'B-negative': 278, ...}\n",
    "    word_label_dict2 = defaultdict(int) # {(LABEL, WORD): COUNT} \n",
    "                               # e.g: {('O', 'All'): 3, ('B-positive', 'food'): 131, ...}\n",
    "\n",
    "    #Populate dictionary for emission 3 with count\n",
    "    for i in range(len(x_train)):\n",
    "        for j in range(len(x_train[i])):\n",
    "            label_dict2[y_train[i][j]] += 1\n",
    "            word_label_dict2[(y_train[i][j], x_train[i][j])] += 1\n",
    "\n",
    "    #Populate dictionary for emission 1 with count\n",
    "    for i in range(len(x_train)):\n",
    "        for j in range(1,len(x_train[i])):\n",
    "            label_dict[y_train[i][j]] += 1\n",
    "            word_label_dict_prev[(y_train[i][j], x_train[i][j-1])] += 1\n",
    "\n",
    "    #Populate dictionary for emission 2 with count\n",
    "    for i in range(len(x_train)):\n",
    "        for j in range(len(x_train[i])-1):\n",
    "            label_dict[y_train[i][j]] += 1\n",
    "            word_label_dict_next[(y_train[i][j], x_train[i][j+1])] += 1\n",
    "\n",
    "\n",
    "    #Populate dictionary of emission 1 with log prob\n",
    "    emission_prev = defaultdict(int)\n",
    "    for k in word_label_dict_prev:\n",
    "        tag = k[0]\n",
    "        string = f\"emission1:{str(k[0])}+{str(k[1])}\"\n",
    "        emission_prev[string] = math.log(float(word_label_dict_prev[k])/label_dict[tag])\n",
    "    # print(f\"emission: {emission}\")\n",
    "\n",
    "    #Populate dictionary of emission 2 with log prob\n",
    "    emission_next = defaultdict(int)\n",
    "    for k in word_label_dict_next:\n",
    "        tag = k[0]\n",
    "        string = f\"emission2:{str(k[0])}+{str(k[1])}\"\n",
    "        emission_next[string] = math.log(float(word_label_dict_next[k])/label_dict[tag])\n",
    "\n",
    "    emission = defaultdict(int)\n",
    "    for k in word_label_dict2:\n",
    "        tag = k[0]\n",
    "        string = f\"emission3:{str(k[0])}+{str(k[1])}\"\n",
    "        emission[string] = math.log(float(word_label_dict2[k])/label_dict2[tag])\n",
    "\n",
    "    # getting transition\n",
    "    yi_dict = defaultdict(int)\n",
    "    yj_dict = defaultdict(int)\n",
    "\n",
    "    yi_dict2 = defaultdict(int)\n",
    "    yj_dict2 = defaultdict(int)\n",
    "    \n",
    "    for i in range(len(x_train)):\n",
    "#         if len(y_train[i]) == 0: # this seems to be useless\n",
    "#             continue\n",
    "        #For second dictionary for original features\n",
    "        yi_dict2['START'] += 1\n",
    "        yj_dict2[('START', y_train[i][0])] += 1\n",
    "        yj_dict2[(y_train[i][-1],'STOP')] += 1\n",
    "\n",
    "\n",
    "         # adding START and STOP tag to each sentence\n",
    "        yi_dict['START'] += 1\n",
    "        yj_dict[('START', y_train[i][0],x_train[i][0])] += 1\n",
    "        yj_dict[(y_train[i][-1],'STOP',None)] += 1\n",
    "\n",
    "        for j in range(1,len(x_train[i])):\n",
    "            yi_dict[y_train[i][j]] += 1\n",
    "            yj_dict[(y_train[i][j-1],y_train[i][j],x_train[i][j])] += 1\n",
    "       \n",
    "            if j<len(x_train[i])-1:\n",
    "                yi_dict2[y_train[i][j]] += 1\n",
    "                yj_dict2[(y_train[i][j],y_train[i][j+1])] += 1\n",
    "        yi_dict2[y_train[i][-1]] += 1\n",
    "            \n",
    "                \n",
    "    transition = defaultdict(int)\n",
    "    for k in yj_dict:\n",
    "        string = f\"transition:{str(k[0])}+{str(k[1])}+{str(k[2])}\"\n",
    "        transition[string] = math.log(float(yj_dict[k])/(yi_dict[k[0]]+1)) \n",
    "        unk_string = f\"transition:{str(k[0])}+{str(k[1])}+{unk}\"\n",
    "        transition[unk_string]= math.log(float(1/yi_dict[k[0]]))\n",
    "    \n",
    "  \n",
    "\n",
    "    #for second dictionary for original features\n",
    "    transition2 = defaultdict(int)\n",
    "    for k in yj_dict2:\n",
    "        string = f\"transition2:{str(k[0])}+{str(k[1])}\"\n",
    "        transition2[string] = math.log(float(yj_dict2[k])/yi_dict2[k[0]])\n",
    "    \n",
    "    if \"transition2:START+STOP\" in transition2:\n",
    "        del transition2[\"transition2:START+STOP\"]\n",
    "\n",
    "    write_feature(emission_prev, output_dir + \"/emission_P5_1.txt\") # save emission 1 dictionary\n",
    "    write_feature(emission_next, output_dir + \"/emission_P5_2.txt\") # save emission 2 dictionary\n",
    "    write_feature(transition, output_dir + \"/transition_P5.txt\") # save transition dictionary\n",
    "    write_feature(emission, output_dir + \"/emission_P5_original.txt\") # save emission dictionary\n",
    "    write_feature(transition2, output_dir + \"/transition_P5_original.txt\") # save transition dictionary\n",
    "\n",
    "    for key in emission_prev:\n",
    "        features[key] = emission_prev[key]\n",
    "    for key in emission_next:\n",
    "        features[key] = emission_next[key]\n",
    "    for key in transition:\n",
    "        features[key] = transition[key]\n",
    "    for key in emission:\n",
    "        features[key] = emission[key]\n",
    "    for key in transition2:\n",
    "        features[key] = transition2[key]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7b560f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = get_feature_dict_part5(x_train, y_train, FEATURE_OUTPUT_DIRECTORY)\n",
    "write_feature(feature_dict, FEATURE_OUTPUT_DIRECTORY + \"/features_P5.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3d9f45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(directory, x, y):\n",
    "    with open(directory, 'w') as f:\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x[i])):\n",
    "                f.write(f\"{x[i][j]} {y[i][j]}\\n\")\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "22506509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_part5(sent, feature_dict): \n",
    "    table = []\n",
    "    for i in range(all_tags_len):\n",
    "        temp = []\n",
    "        for j in range(len(sent)):\n",
    "            temp.append(-2**31)\n",
    "        table.append(temp)\n",
    "            \n",
    "    trace = []\n",
    "    for j in range(all_tags_len):\n",
    "        temp = []\n",
    "        for i in range(len(sent)):\n",
    "            temp.append(None)\n",
    "        trace.append(temp)\n",
    "    \n",
    "    score = -2**31\n",
    "    pointer = None\n",
    "    \n",
    "    # START -> first tag\n",
    "    # check first word sent[0]\n",
    "    for i in range(all_tags_len):\n",
    "        n = len(sent)\n",
    "        trace[i][0] = 'START'\n",
    "        if n!=1:\n",
    "            \n",
    "            if (f\"transition:START+{ALL_TAGS[i]}+{sent[0]}\" in feature_dict):\n",
    "                if (f\"emission2:{ALL_TAGS[i]}+{sent[1]}\" in feature_dict):\n",
    "                    table[i][0] = feature_dict[f\"transition:START+{ALL_TAGS[i]}+{sent[0]}\"] +\\\n",
    "                                feature_dict[f\"emission2:{ALL_TAGS[i]}+{sent[1]}\"] +\\\n",
    "                                            feature_dict[f\"transition2:START+{ALL_TAGS[i]}\"] +\\\n",
    "                                            feature_dict[f\"emission3:{ALL_TAGS[i]}+{sent[0]}\"]\n",
    "\n",
    "                else:\n",
    "                    table[i][0] = feature_dict[f\"transition:START+{ALL_TAGS[i]}+{sent[0]}\"] +\\\n",
    "                                                feature_dict[f\"transition2:START+{ALL_TAGS[i]}\"] +\\\n",
    "                                                feature_dict[f\"emission3:{ALL_TAGS[i]}+{sent[0]}\"]\n",
    "          \n",
    "            else:\n",
    "                if (f\"emission2:{ALL_TAGS[i]}+{sent[1]}\" in feature_dict):\n",
    "                    # table[i][0] = feature_dict[f\"transition:START+{ALL_TAGS[i]}+{unk}\"] +\\\n",
    "                    table[i][0] =feature_dict[f\"emission2:{ALL_TAGS[i]}+{sent[1]}\"] +\\\n",
    "                                                 feature_dict[f\"transition2:START+{ALL_TAGS[i]}\"]\n",
    "                                                # feature_dict[f\"emission3:{ALL_TAGS[i]}+{unk}\"]\n",
    "                else:\n",
    "                    # table[i][0] = feature_dict[f\"transition:START+{ALL_TAGS[i]}+{unk}\"] +\\\n",
    "                    table[i][0] =   feature_dict[f\"transition2:START+{ALL_TAGS[i]}\"] \n",
    "                                            # feature_dict[f\"emission3:{ALL_TAGS[i]}+{unk}\"]\n",
    "        else:\n",
    "            table[i][0] = feature_dict[f\"transition:START+{ALL_TAGS[i]}+{sent[0]}\"] +\\\n",
    "                                         feature_dict[f\"transition2:START+{ALL_TAGS[i]}\"] +\\\n",
    "                                            feature_dict[f\"emission3:{ALL_TAGS[i]}+{sent[0]}\"]\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "    # iterate through the rest of sent\n",
    "    for s in range(1, len(sent)):\n",
    "        for v in range(all_tags_len):\n",
    "            \n",
    "            # transition score u -> v\n",
    "            for u in range(all_tags_len):\n",
    "                if (f\"transition:{ALL_TAGS[u]}+{ALL_TAGS[v]}+{sent[s]}\"in feature_dict):\n",
    "                    if table[v][s] < table[u][s-1] + feature_dict[f\"transition:{ALL_TAGS[u]}+{ALL_TAGS[v]}+{sent[s]}\"] + feature_dict[f\"transition2:{ALL_TAGS[u]}+{ALL_TAGS[v]}\"]:\n",
    "                        table[v][s] = table[u][s-1] + feature_dict[f\"transition:{ALL_TAGS[u]}+{ALL_TAGS[v]}+{sent[s]}\"] + feature_dict[f\"transition2:{ALL_TAGS[u]}+{ALL_TAGS[v]}\"]\n",
    "                        trace[v][s] = u\n",
    "                else:\n",
    "                    if table[v][s] < table[u][s-1] +  feature_dict[f\"transition2:{ALL_TAGS[u]}+{ALL_TAGS[v]}\"]:\n",
    "                        table[v][s] = table[u][s-1] +  feature_dict[f\"transition2:{ALL_TAGS[u]}+{ALL_TAGS[v]}\"]\n",
    "                        trace[v][s] = u\n",
    "\n",
    "            if (f\"emission3:{ALL_TAGS[v]}+{sent[s]}\" in feature_dict):\n",
    "                table[v][s] += feature_dict[f\"emission3:{ALL_TAGS[v]}+{sent[s]}\"]    \n",
    "            \n",
    "\n",
    "            if s!=len(sent)-1:\n",
    "                if (f\"emission2:{ALL_TAGS[v]}+{sent[s+1]}\" in feature_dict) and (f\"emission1:{ALL_TAGS[v]}+{sent[s-1]}\" in feature_dict):\n",
    "                    table[v][s] += feature_dict[f\"emission2:{ALL_TAGS[v]}+{sent[s+1]}\"] + feature_dict[f\"emission1:{ALL_TAGS[v]}+{sent[s-1]}\"]\n",
    "                elif (f\"emission2:{ALL_TAGS[v]}+{sent[s+1]}\" in feature_dict):\n",
    "                    table[v][s] += feature_dict[f\"emission2:{ALL_TAGS[v]}+{sent[s+1]}\"]\n",
    "                elif  (f\"emission2:{ALL_TAGS[v]}+{sent[s-1]}\" in feature_dict):\n",
    "                    table[v][s] +=feature_dict[f\"emission1:{ALL_TAGS[v]}+{sent[s-1]}\"]\n",
    "            else:\n",
    "                if (f\"emission1:{ALL_TAGS[v]}+{sent[s-1]}\" in feature_dict):\n",
    "                    table[v][s] += feature_dict[f\"emission1:{ALL_TAGS[v]}+{sent[s-1]}\"]\n",
    "        \n",
    "\n",
    "    \n",
    "    # last word -> STOP\n",
    "    for i in range(all_tags_len):\n",
    "        \n",
    "\n",
    "        if score < table[i][-1] + feature_dict[f\"transition:{ALL_TAGS[i]}+STOP+{None}\"] + feature_dict[f\"transition2:{ALL_TAGS[i]}+STOP\"]:\n",
    "            score = table[i][-1] + feature_dict[f\"transition:{ALL_TAGS[i]}+STOP+{None}\"] + feature_dict[f\"transition2:{ALL_TAGS[i]}+STOP\"]\n",
    "            pointer = i\n",
    "    \n",
    "    \n",
    "    output = ['STOP']\n",
    "    output.append(ALL_TAGS[pointer])\n",
    "    wanted_tag = pointer\n",
    "    \n",
    "    for i in range(len(sent)-1, 0, -1):\n",
    "        output.append(ALL_TAGS[trace[wanted_tag][i]])\n",
    "        wanted_tag = trace[wanted_tag][i]\n",
    "    \n",
    "    output.append('START')\n",
    "    return output[::-1], score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6879b9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['START', 'O', 'B-positive', 'O', 'O', 'O', 'B-positive', 'O', 'STOP'],\n",
       " -163.96888670538772)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_part5('Great food with an awesome atmosphere !'.split(), feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7848105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_validation_file(directory):\n",
    "    dev = []\n",
    "    with open(directory) as f:\n",
    "        s = []\n",
    "        for line in f:\n",
    "            if line == '\\n':\n",
    "                dev.append(s)\n",
    "                s = []\n",
    "            else:\n",
    "                temp = line.strip()\n",
    "                s.append(temp)\n",
    "    return dev\n",
    "x_dev = read_validation_file(DATA_FILE +'/dev.in')\n",
    "y_dev = []\n",
    "\n",
    "for i in range(len(x_dev)):\n",
    "    output,_ = viterbi_part5(x_dev[i], feature_dict)\n",
    "    y_dev.append(output[1:-1]) #remove START, STOP\n",
    "\n",
    "write_output(DATA_FILE +'/dev.p5.out', x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c113ce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(n):\n",
    "    _max = np.max(n)\n",
    "    i = n - _max\n",
    "    _sum = np.exp(i).sum()\n",
    "    return np.log(_sum) + _max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "29fb0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_part5(sent, feature_dict):\n",
    "    alpha = []\n",
    "    for j in range(all_tags_len):\n",
    "        temp = []\n",
    "        for i in range(len(sent)):\n",
    "            temp.append(0)\n",
    "        alpha.append(temp)\n",
    "    \n",
    "    # START -> first tag\n",
    "    # check first word sent[0]\n",
    "    \n",
    "    for i in range(all_tags_len):\n",
    "        \n",
    "        if len(sent)>1:\n",
    "            alpha[i][0] =feature_dict[f\"transition:START+{ALL_TAGS[i]}+{sent[0]}\"] +\\\n",
    "                        feature_dict[f\"emission2:{ALL_TAGS[i]}+{sent[1]}\"] +\\\n",
    "                                    feature_dict[f\"transition2:START+{ALL_TAGS[i]}\"] +\\\n",
    "                                    feature_dict[f\"emission3:{ALL_TAGS[i]}+{sent[0]}\"]\n",
    "        else:\n",
    "            alpha[i][0] =feature_dict[f\"transition:START+{ALL_TAGS[i]}+{sent[0]}\"] +\\\n",
    "                                    feature_dict[f\"transition2:START+{ALL_TAGS[i]}\"] +\\\n",
    "                                    feature_dict[f\"emission3:{ALL_TAGS[i]}+{sent[0]}\"]\n",
    "        \n",
    "    # iterate through the rest of sent\n",
    "    for i in range(1, len(sent)):\n",
    "        for v in range(all_tags_len):\n",
    "            temp = np.zeros(all_tags_len)\n",
    "            \n",
    "            # transition score u -> v\n",
    "            for u in range(all_tags_len):\n",
    "               \n",
    "                temp[u] = alpha[u][i-1] +\\\n",
    "                feature_dict[f\"transition:{ALL_TAGS[u]}+{ALL_TAGS[v]}+{sent[i]}\"] +\\\n",
    "                feature_dict[f\"emission1:{ALL_TAGS[v]}+{sent[i-1]}\"] +\\\n",
    "                                feature_dict[f\"transition2:{ALL_TAGS[u]}+{ALL_TAGS[v]}\"] +\\\n",
    "                                feature_dict[f\"emission3:{ALL_TAGS[v]}+{sent[i]}\"]\n",
    "                if i<len(sent)-1:\n",
    "                    temp[u] += feature_dict[f\"emission2:{ALL_TAGS[v]}+{sent[i+1]}\"]     \n",
    "            alpha[v][i] = log_sum_exp(temp)\n",
    "    \n",
    "    # last word -> STOP\n",
    "    temp = np.zeros(all_tags_len)\n",
    "    for i in range(all_tags_len):\n",
    "       \n",
    "        temp[i] = alpha[i][-1] + feature_dict[f\"transition:{ALL_TAGS[i]}+STOP+{None}\"] +\\\n",
    "                                feature_dict[f\"transition2:{ALL_TAGS[i]}+STOP\"]\n",
    "    \n",
    "    return alpha, log_sum_exp(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c157258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_part5(sent, feature_dict):\n",
    "    beta = []\n",
    "    for i in range(all_tags_len):\n",
    "        temp = []\n",
    "        for j in range(len(sent)):\n",
    "            temp.append(0)\n",
    "        beta.append(temp)\n",
    "    \n",
    "    for i in range(all_tags_len):\n",
    "        beta[i][-1] = feature_dict[f\"transition:{ALL_TAGS[i]}+STOP+{None}\"] +\\\n",
    "                                    feature_dict[f\"transition2:{ALL_TAGS[i]}+STOP\"]\n",
    "    \n",
    "    for i in range(len(sent)-2, -1, -1):\n",
    "        for v in range(all_tags_len):\n",
    "            temp = np.zeros(all_tags_len)\n",
    "            \n",
    "            for u in range(all_tags_len):\n",
    "              \n",
    "                temp[u] = beta[u][i+1] +\\\n",
    "                           feature_dict[f\"transition:{ALL_TAGS[v]}+{ALL_TAGS[u]}+{sent[i+1]}\"]+\\\n",
    "                           feature_dict[f\"emission1:{ALL_TAGS[u]}+{sent[i]}\"]+\\\n",
    "                                    feature_dict[f\"transition2:{ALL_TAGS[v]}+{ALL_TAGS[u]}\"] +\\\n",
    "                                    feature_dict[f\"emission3:{ALL_TAGS[u]}+{sent[i+1]}\"]\n",
    "                if i<len(sent)-2:\n",
    "                           temp[u] += feature_dict[f\"emission2:{ALL_TAGS[u]}+{sent[i+2]}\"]\n",
    "                           \n",
    "                \n",
    "            beta[v][i] = log_sum_exp(temp)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e62aff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_part5(feature_dict, x, y):\n",
    "    score = 0\n",
    "    score += feature_dict[f\"transition:START+{y[0]}+{x[0]}\"] +\\\n",
    "                            feature_dict[f\"transition2:START+{y[0]}\"]\n",
    "\n",
    "    \n",
    "    #Original feature\n",
    "    if f\"emission3:{y[0]}+{x[0]}\" in feature_dict:\n",
    "        score += feature_dict[f\"emission3:{y[0]}+{x[0]}\"]\n",
    "\n",
    "    if len(x)>1:\n",
    "        if f\"emission2:{y[0]}+{x[1]}\" in feature_dict:\n",
    "            score += feature_dict[f\"emission2:{y[0]}+{x[1]}\"]\n",
    "    \n",
    "    for i in range(1, len(x)):\n",
    "        score += feature_dict[f\"transition:{y[i-1]}+{y[i]}+{x[i]}\"]\n",
    "\n",
    "        #original feature emission\n",
    "        if f\"emission:{y[i]}+{x[i]}\" in feature_dict:\n",
    "            score += feature_dict[f\"emission3:{y[i]}+{x[i]}\"]\n",
    "\n",
    "        #Original feature transmission\n",
    "        score += feature_dict[f\"transition2:{y[i-1]}+{y[i]}\"]\n",
    "        \n",
    "        if f\"emission1:{y[i]}+{x[i-1]}\" in feature_dict:\n",
    "            score += feature_dict[f\"emission1:{y[i]}+{x[i-1]}\"]\n",
    "\n",
    "        if i<len(x)-1:\n",
    "            if f\"emission2:{y[i]}+{x[i+1]}\" in feature_dict:\n",
    "                score += feature_dict[f\"emission2:{y[i]}+{x[i+1]}\"]\n",
    "\n",
    "    score += feature_dict[f\"transition:{y[-1]}+STOP+{None}\"]\n",
    "    \n",
    "    #original feature transmission STOP\n",
    "    score += feature_dict[f\"transition2:{y[-1]}+STOP\"]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c2355cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-133.73742006205444"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_score_part5(feature_dict,\n",
    "              'Great food with an awesome atmosphere !'.split(),\n",
    "              'O B-positive O O O B-positive O'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "56294818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_part5(X_train, y_train, feature_dict):\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        gold_score = compute_score_part5(feature_dict, X_train[i], y_train[i])\n",
    "        _, total_score = forward_part5(X_train[i], feature_dict)\n",
    "        loss += gold_score - total_score\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a11ed6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with features from part 1: -147875.36879758135\n"
     ]
    }
   ],
   "source": [
    "temp_loss = compute_loss_part5(x_train, y_train, feature_dict)\n",
    "print('Loss with features from part 1:', temp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3d0c075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_soft_count_part5(sent, feature_dict, alpha, beta, score):\n",
    "    result = defaultdict(int)\n",
    "    for k in feature_dict:\n",
    "        result[k] = 0\n",
    "    \n",
    "    # transition\n",
    "    for i in range(all_tags_len):\n",
    "        update = alpha[i][0] + beta[i][0] - score\n",
    "\n",
    "        #original\n",
    "        result[f\"transition2:START+{ALL_TAGS[i]}\"] += np.exp(update)\n",
    "\n",
    "        result[f\"transition:START+{ALL_TAGS[i]}+{sent[0]}\"] += np.exp(update)\n",
    "    \n",
    "    for i in range(1, len(sent)):\n",
    "        for u in range(all_tags_len):\n",
    "            for v in range(all_tags_len):\n",
    "                string2 = f\"transition2:{ALL_TAGS[v]}+{ALL_TAGS[u]}\"\n",
    "                string = f\"transition:{ALL_TAGS[v]}+{ALL_TAGS[u]}+{sent[i]}\"\n",
    "                update = alpha[v][i-1] \\\n",
    "                                + feature_dict[string] \\\n",
    "                                + feature_dict[f\"emission1:{ALL_TAGS[u]}+{sent[i-1]}\"] \\\n",
    "                                + beta[u][i] - score \\\n",
    "                                        + feature_dict[string2]\n",
    "                if i<len(sent)-1:\n",
    "                    update += feature_dict[f\"emission2:{ALL_TAGS[u]}+{sent[i+1]}\"]\n",
    "                result[string] += np.exp(update)\n",
    "                result[string2] += np.exp(update)\n",
    "\n",
    "    for i in range(all_tags_len):\n",
    "        update = alpha[i][-1] + beta[i][-1] - score\n",
    "        result[f\"transition:{ALL_TAGS[i]}+STOP+{None}\"] += np.exp(update)\n",
    "        result[f\"transition2:{ALL_TAGS[i]}+STOP\"] += np.exp(update)\n",
    "    \n",
    "    # emission\n",
    "    for i in range(len(sent)):\n",
    "        for j in range(all_tags_len):\n",
    "            string = f\"emission1:{ALL_TAGS[j]}+{sent[i-1]}\"\n",
    "            update = alpha[j][i] + beta[j][i] - score\n",
    "            result[string] += np.exp(update)\n",
    "    for i in range(len(sent)):\n",
    "        for j in range(all_tags_len):\n",
    "            if i < len(sent)-1:\n",
    "                string = f\"emission2:{ALL_TAGS[j]}+{sent[i+1]}\"\n",
    "                update = alpha[j][i] + beta[j][i] - score\n",
    "            result[string] += np.exp(update)\n",
    "\n",
    "    # original emission\n",
    "    for i in range(len(sent)):\n",
    "        for j in range(all_tags_len):\n",
    "            string = f\"emission3:{ALL_TAGS[j]}+{sent[i]}\"\n",
    "            update = alpha[j][i] + beta[j][i] - score\n",
    "            result[string] += np.exp(update)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "629a0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hard_count_part5(x, y, feature_dict):\n",
    "    result = defaultdict(int)\n",
    "    for k in feature_dict:\n",
    "        result[k] = 0\n",
    "    \n",
    "    # start\n",
    "    result[f\"transition:START+{y[0]}+{x[0]}\"] += 1\n",
    "    if len(x)>1:\n",
    "        result[f\"emission2:{y[0]}+{x[1]}\"] += 1\n",
    "    \n",
    "    # recursive\n",
    "    for i in range(1, len(x)):\n",
    "        result[f\"transition:+{y[i-1]}+{y[i]}+{x[i]}\"] += 1\n",
    "        result[f\"emission1:{y[i]}+{x[i-1]}\"] += 1\n",
    "        if i<len(x)-1:\n",
    "            result[f\"emission2:{y[i]}+{x[i+1]}\"] += 1\n",
    "    \n",
    "    # end\n",
    "    result[f\"transition:{y[-1]}+STOP+{None}\"] += 1\n",
    "\n",
    "\n",
    "\n",
    "    #Original Features\n",
    "     # start\n",
    "    result[f\"transition2:START+{y[0]}\"] += 1\n",
    "    result[f\"emission3:{y[0]}+{x[0]}\"] += 1\n",
    "    \n",
    "    # recursive\n",
    "    for i in range(1, len(x)):\n",
    "        result[f\"transition2:+{y[i-1]}+{y[i]}\"] += 1\n",
    "        result[f\"emission3:{y[i]}+{x[i]}\"] += 1\n",
    "    \n",
    "    # end\n",
    "    result[f\"transition2:{y[-1]}+STOP\"] += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4d9391b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_index_mapping(feat_dict):\n",
    "    feat_mapping = {}\n",
    "    index_mapping = {}\n",
    "    index = 0\n",
    "    \n",
    "    for key in feat_dict:\n",
    "        feat_mapping[key] = index\n",
    "        index_mapping[index] = key\n",
    "        index += 1\n",
    "        \n",
    "    return feat_mapping, index_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b4d1b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_and_grad_part5(x, y, feature_dict):\n",
    "    grad_dict = defaultdict(int)\n",
    "    for k in feature_dict:\n",
    "        grad_dict[k] = 0\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        alpha, forward_score = forward_part5(x[i], feature_dict)\n",
    "        beta = backward_part5(x[i], feature_dict)\n",
    "        \n",
    "        # loss\n",
    "        expected_result = compute_score_part5(feature_dict, x[i], y[i])\n",
    "        loss += expected_result - forward_score\n",
    "        \n",
    "        # gradient\n",
    "        soft_dict = compute_soft_count_part5(x[i], feature_dict, alpha, beta, forward_score)\n",
    "        hard_dict = compute_hard_count_part5(x[i], y[i], feature_dict)\n",
    "        for feat in feature_dict:\n",
    "            update = soft_dict[feat] - hard_dict[feat]\n",
    "            grad_dict[feat] += update\n",
    "    \n",
    "    return -loss, grad_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "071baa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m, s = divmod(t, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return f\"{h:.0f}h {m:.0f}m {s:.2f}s\"\n",
    "\n",
    "def now():\n",
    "    tz = pytz.timezone('Asia/Singapore')\n",
    "    now = datetime.now(tz)\n",
    "    return datetime.strftime(now, \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f160eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_grad(w, *args):\n",
    "    '''\n",
    "    This function will be called by \"fmin_l_bfgs_b\"\n",
    "    Arg:\n",
    "    w: weights, numpy array\n",
    "    Returns:\n",
    "    loss: loss, float\n",
    "    grads: gradients, numpy array\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    grads = np.zeros(len(w))\n",
    "    x, y  = args\n",
    "    \n",
    "    features_dict = {}\n",
    "    for i in range(len(w)):\n",
    "        features_dict[index_mapping[i]] = w[i]\n",
    "        \n",
    "    loss, grad_dict = compute_loss_and_grad_part5(x, y, features_dict)\n",
    "    \n",
    "    # loss with reg\n",
    "    loss += LEARN_CO * np.sum(w**2)\n",
    "    \n",
    "    # grad with reg\n",
    "    for key in grad_dict:\n",
    "        grads[feat_mapping[key]] = grad_dict[key] + 2 * LEARN_CO * w[feat_mapping[key]]\n",
    "    \n",
    "    \n",
    "    print(f\"Now:\\t\\t{now()}\")\n",
    "    print(f\"loss:\\t\\t{loss:.5f}\")\n",
    "    print(f\"time taken for computing loss:\\t{format_time(time.time() - start_time)}\")\n",
    "    print(f\"overall time taken so far:\\t{format_time(time.time() - OVERALL_START)}\")\n",
    "    print(\"\\n\")\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2d473b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Start training CRF--------------------\n",
      "Now:\t\t2022-08-12 12:32:29\n",
      "loss:\t\t52775.02915\n",
      "time taken for computing loss:\t0h 14m 13.68s\n",
      "overall time taken so far:\t0h 14m 15.23s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 12:46:29\n",
      "loss:\t\t50169.75871\n",
      "time taken for computing loss:\t0h 14m 0.02s\n",
      "overall time taken so far:\t0h 28m 15.46s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 13:00:41\n",
      "loss:\t\t43394.44239\n",
      "time taken for computing loss:\t0h 14m 12.48s\n",
      "overall time taken so far:\t0h 42m 28.01s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 13:15:14\n",
      "loss:\t\t77099.17454\n",
      "time taken for computing loss:\t0h 14m 32.09s\n",
      "overall time taken so far:\t0h 57m 0.18s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 13:29:53\n",
      "loss:\t\t47592.07140\n",
      "time taken for computing loss:\t0h 14m 39.38s\n",
      "overall time taken so far:\t1h 11m 39.62s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 13:49:59\n",
      "loss:\t\t44006.43311\n",
      "time taken for computing loss:\t0h 20m 6.32s\n",
      "overall time taken so far:\t1h 31m 46.00s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 14:04:08\n",
      "loss:\t\t43488.11777\n",
      "time taken for computing loss:\t0h 14m 8.73s\n",
      "overall time taken so far:\t1h 45m 54.80s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 14:18:32\n",
      "loss:\t\t43408.90072\n",
      "time taken for computing loss:\t0h 14m 23.26s\n",
      "overall time taken so far:\t2h 0m 18.12s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 14:37:40\n",
      "loss:\t\t43396.67688\n",
      "time taken for computing loss:\t0h 19m 8.56s\n",
      "overall time taken so far:\t2h 19m 26.74s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 14:52:18\n",
      "loss:\t\t43394.78779\n",
      "time taken for computing loss:\t0h 14m 38.02s\n",
      "overall time taken so far:\t2h 34m 4.83s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 15:07:03\n",
      "loss:\t\t43394.49579\n",
      "time taken for computing loss:\t0h 14m 44.35s\n",
      "overall time taken so far:\t2h 48m 49.24s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 15:22:00\n",
      "loss:\t\t43394.45065\n",
      "time taken for computing loss:\t0h 14m 57.75s\n",
      "overall time taken so far:\t3h 3m 47.06s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 15:36:13\n",
      "loss:\t\t43394.44367\n",
      "time taken for computing loss:\t0h 14m 12.02s\n",
      "overall time taken so far:\t3h 17m 59.14s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 15:50:27\n",
      "loss:\t\t43394.44259\n",
      "time taken for computing loss:\t0h 14m 13.94s\n",
      "overall time taken so far:\t3h 32m 13.14s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 16:05:26\n",
      "loss:\t\t43394.44242\n",
      "time taken for computing loss:\t0h 14m 59.23s\n",
      "overall time taken so far:\t3h 47m 12.43s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 16:22:15\n",
      "loss:\t\t43394.44240\n",
      "time taken for computing loss:\t0h 16m 49.45s\n",
      "overall time taken so far:\t4h 4m 1.96s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 16:38:06\n",
      "loss:\t\t43394.44239\n",
      "time taken for computing loss:\t0h 15m 50.61s\n",
      "overall time taken so far:\t4h 19m 52.64s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 16:52:40\n",
      "loss:\t\t43394.44239\n",
      "time taken for computing loss:\t0h 14m 34.37s\n",
      "overall time taken so far:\t4h 34m 27.07s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 17:07:05\n",
      "loss:\t\t43394.44239\n",
      "time taken for computing loss:\t0h 14m 24.55s\n",
      "overall time taken so far:\t4h 48m 51.68s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 17:21:22\n",
      "loss:\t\t43394.44239\n",
      "time taken for computing loss:\t0h 14m 16.90s\n",
      "overall time taken so far:\t5h 3m 8.65s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 17:36:09\n",
      "loss:\t\t43394.44239\n",
      "time taken for computing loss:\t0h 14m 47.11s\n",
      "overall time taken so far:\t5h 17m 55.82s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 17:51:00\n",
      "loss:\t\t43394.44239\n",
      "time taken for computing loss:\t0h 14m 50.54s\n",
      "overall time taken so far:\t5h 32m 46.43s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 18:04:08\n",
      "loss:\t\t43394.44239\n",
      "time taken for computing loss:\t0h 13m 8.34s\n",
      "overall time taken so far:\t5h 45m 54.83s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 18:16:09\n",
      "loss:\t\t17328444.90839\n",
      "time taken for computing loss:\t0h 12m 0.85s\n",
      "overall time taken so far:\t5h 57m 55.77s\n",
      "\n",
      "\n",
      "Now:\t\t2022-08-12 18:29:13\n",
      "loss:\t\t43394.44239\n",
      "time taken for computing loss:\t0h 13m 3.67s\n",
      "overall time taken so far:\t6h 10m 59.50s\n",
      "\n",
      "\n",
      "final loss:\t\t43394.442393330304\n",
      "--------------------Training finished--------------------\n",
      "TOTAL TIME TAKEN: 6h 10m 59.61s\n"
     ]
    }
   ],
   "source": [
    "feat_mapping, index_mapping = feat_index_mapping(feature_dict)\n",
    "\n",
    "print(\"-\"*20 + \"Start training CRF\" + \"-\"*20)\n",
    "OVERALL_START = time.time()\n",
    "\n",
    "opt_w, final_loss, _ = fmin_l_bfgs_b(get_loss_grad,\n",
    "                 x0 = np.zeros(len(index_mapping)), #initial weight guess\n",
    "                 pgtol = 0.1,\n",
    "                 args = (x_train, y_train))\n",
    "\n",
    "print(f\"final loss:\\t\\t{final_loss}\")\n",
    "\n",
    "print(\"-\"*20 + 'Training finished' + \"-\"*20)\n",
    "\n",
    "TT = time.time() - OVERALL_START\n",
    "print(f\"TOTAL TIME TAKEN: {format_time(TT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "aa48a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_feat = {}\n",
    "for i in range(len(opt_w)):\n",
    "    pred_feat[index_mapping[i]] = opt_w[i]\n",
    "\n",
    "x_dev = read_validation_file(DATA_FILE + '/dev.in')\n",
    "y_dev = []\n",
    "\n",
    "for i in range(len(x_dev)):\n",
    "    output, _ = viterbi_part5(x_dev[i], pred_feat)\n",
    "    y_dev.append(output[1:-1])\n",
    "\n",
    "write_output(DATA_FILE + '/dev.p5.out', x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a472e",
   "metadata": {},
   "source": [
    "# Part 5 Results\n",
    "processed 3809 tokens with 44 phrases; found: 210 phrases; correct: 14.\n",
    "<br />\n",
    "accuracy:  50.00%; (non-O)\n",
    "<br />\n",
    "accuracy:  92.41%; precision:   6.67%; recall:  31.82%; FB1:  11.02\n",
    "<br />\n",
    "         negative: precision:   0.00%; recall:   0.00%; FB1:   0.00  65\n",
    "         <br />\n",
    "          neutral: precision:   0.00%; recall:   0.00%; FB1:   0.00  8<br />\n",
    "         positive: precision:  10.22%; recall:  32.56%; FB1:  15.56  137<br />\n",
    "((6.666666666666667, 31.818181818181817, 11.023622047244094), 0)<br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcf211192b8305e65baef152b0c6fa0a02f332f050fb0f887b1262c425357821"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
